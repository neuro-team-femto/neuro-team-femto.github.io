---
title: Publication
permalink: /publications/
---

We care about [open science](https://en.wikipedia.org/wiki/Open_science). Because our research is almost entirely publicly funded, and because all our permanent and non-permanent positions are civil servants [part of the public sector](https://en.wikipedia.org/wiki/French_National_Centre_for_Scientific_Research), we believe it is a moral commitment to make our results accessible for free at all levels of society. This includes publications (open-access) but also research material, stimuli, data, and software. 

Click on the <img style='display:inline;position: relative;top: 10px' height='20' src='/images/site_icons/access.jpg'> banners below for direct access to the PDFs of the papers; <br> 
on <img style='display:inline;position: relative;top: 10px' height='20' src='/images/site_icons/code.jpg'> for a link to the project's code hosted on the group's [github page](github.com/neuro-team-femto); <br>
and on <img style='display:inline;position: relative;top: 10px' height='20' src='/images/site_icons/data.jpg'> for a link to the associated research data (stimuli and results). 

Our overt objective with the following is that 100% of our publications are associated with all three banners. If one is missing for a specific project, there's a good chance that we're working on it - do feel free to request as you need. 

<!-- Since 2021, all our research data is doi-indexed in the [Université de Bourgogne Franche-Comté](https://www.ubfc.fr/)'s Open Data portal [Dat@UBFC](https://dataosu.obs-besancon.fr). -->
In addition to resources specific to papers below, we also develop and maintain a number of open-source research software, including reverse-correlation toolbox [CLEESE](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0205943) and vocal-feedback plateform [DAVID](https://link.springer.com/article/10.3758/s13428-017-0873-y), which can be found on our [Resources page]({{site.baseurl}}/resources).  


### 2025

[_Learning the bistable cortical dynamics of the sleep-onset period_](https://www.biorxiv.org/content/10.1101/2025.07.17.665340v1) <br>
*Hu, Z.*, *Aravind, M.*, Lei, X., Kutz, J. N., *Aucouturier, JJ* <br>
Biorxiv, 2025.07.17.665340<br>
[<img style='display:inline;padding-top: 5px;' height='25' src='/images/site_icons/access.jpg'>]({{site.baseurl}}/articles/2025/hu_biorxiv_2025.pdf)

[_The social transfer function: how dynamic predictions of facial consequences drive judgements of social contingency_](https://osf.io/preprints/osf/v43hp_v1) <br>
*Guha, R.*, Arias-Sarah, *Aucouturier, JJ* <br>
OSF preprint, v43hp_v1<br>
[<img style='display:inline;padding-top: 5px;' height='25' src='/images/site_icons/access.jpg'>]({{site.baseurl}}/articles/2025/guha_osf_2025.pdf)

_Reverse-correlation modeling of deficits of prosody perception in right-hemisphere stroke_ <br>
*Adl Zarrabi, A.* <br>
PhD Thesis, Université Marie et Louis Pasteur, 2025<br>
[<img style='display:inline;padding-top: 5px;' height='25' src='/images/site_icons/access.jpg'>]({{site.baseurl}}/articles/2025/adl_zarrabi_phd_2025.pdf)


<hr>

### 2024

[_Aligning the smiles of dating dyads causally increases attraction_](https://www.pnas.org/doi/10.1073/pnas.2400369121) <br>
Arias-Sarah, P., Bedoya, D., Daube, C., *Aucouturier, JJ.*, Hall, L., Johansson, P. <br>
Proceedings of the National Academy of Sciences 121 (45), e2400369121<br>
[<img style='display:inline;padding-top: 5px;' height='25' src='/images/site_icons/access.jpg'>]({{site.baseurl}}/articles/2024/arias_pnas_2024.pdf)

[_Emotional contagion to vocal smile revealed by combined pupil reactivity and motor resonance_](https://www.nature.com/articles/s41598-024-74848-w)
Merchie, A., Ranty, Z., Aguillon-Hernandez, N., Aucouturier,  JJ., Wardak, C., Gomot, M. <br>
Scientific Reports 14 (1), 25043<br>
[<img style='display:inline;padding-top: 5px;' height='25' src='/images/site_icons/access.jpg'>]({{site.baseurl}}/articles/2024/merchie_scirep_2024.pdf)

[_Neural adaptation to changes in self-voice during puberty_](https://www.cell.com/trends/neurosciences/abstract/S0166-2236(24)00142-5) <br>
Pinheiro, A., *Aucouturier, JJ* & Kotz, Sonja. <br>
Trends in Neurosciences, August 2024. <br>
[<img style='display:inline;padding-top: 5px;' height='25' src='/images/site_icons/access.jpg'>]({{site.baseurl}}/articles/2024/pinheiro_TiN_2024.pdf)

[_Cortical responses to looming sources are explained away by the auditory periphery_](https://www.sciencedirect.com/science/article/pii/S0010945224001692) <br>
*Benghanem, S.*, *Guha, R.*, *Pruvost-Robieux, E.*, Levi-Strauss, J., *Joucla, C.*, Cariou, A., Gavaret, M. & *Aucouturier, J. J*. <br> 
Cortex. Vol. 177, 2024. <br>
[<img style='display:inline;padding-top: 5px;' height='25' src='/images/site_icons/access.jpg'>]({{site.baseurl}}/articles/2024/benghanem_cortex_2024.pdf)
[<img style='display:inline;padding-top: 5px;' height='25' src='/images/site_icons/data.jpg'>](https://osf.io/wjsbf/)
[<img style='display:inline;padding-top: 5px;' height='25' src='/images/site_icons/code.jpg'>](https://github.com/neuro-team-femto/looming)

[_A simple psychophysical procedure separates representational and noise components in impairments of speech prosody perception after right-hemisphere stroke_](https://www.nature.com/articles/s41598-024-64295-y) <br>
*Adl Zarrabi, A.*, Jeulin, M., Bardet, P., Commère, P., Naccache, L., *Aucouturier, J. J.* & *Villain, M.*. <br>
Scientific Reports volume 14 (15194), 2024. <br>
[<img style='display:inline;padding-top: 5px;' height='25' src='/images/site_icons/access.jpg'>]({{site.baseurl}}/articles/2024/adlzarrabi_scirep_2024.pdf)
[<img style='display:inline;padding-top: 5px;' height='25' src='/images/site_icons/data.jpg'>](https://github.com/neuro-team-femto/revcor_avc)
[<img style='display:inline;padding-top: 5px;' height='25' src='/images/site_icons/code.jpg'>](https://github.com/neuro-team-femto/revcor_avc)

[_Mmm whatcha say? Uncovering distal and proximal context effects in first and second-language word perception using psychophysical reverse correlation_](https://rosielab.github.io/vocal_ambiguity/)<br>
*Tuttösi, P.*, Yeung, H.H., Wang, Y., Wang, F., *Denis, G.*, *Aucouturier, JJ.* & Lim, A. <br>
Interspeech, 2024. <br>
[<img style='display:inline;padding-top: 5px;' height='25' src='/images/site_icons/access.jpg'>]({{site.baseurl}}/articles/2024/tuttosi_interspeech_2024.pdf)
[<img style='display:inline;padding-top: 5px;' height='25' src='/images/site_icons/data.jpg'>](https://github.com/neuro-team-femto/vocal_ambiguity)
[<img style='display:inline;padding-top: 5px;' height='25' src='/images/site_icons/code.jpg'>](https://github.com/neuro-team-femto/vocal_ambiguity)

[_Social affective inferences in the era of AI filters: towards the Bayesian reshaping of human sociality?_](https://osf.io/preprints/osf/k5fdr)<br>
*Guerouaou, N.*, Vaiva, G. & *Aucouturier, JJ.* <br>
OSF Preprints, 2024. <br>
[<img style='display:inline;padding-top: 5px;' height='25' src='/images/site_icons/access.jpg'>]({{site.baseurl}}/articles/2024/guerouaou_osf_2024.pdf)

[_Intact Representation of Vocal Smile in Autism: A reverse correlation approach_](https://osf.io/preprints/psyarxiv/q4n7z)<br>
Merchie, A., Ranty, Z., *Adl Zarrabi, A.*, Bonnet-Brilhault, F., Houy-Durand, E., Aucouturier, JJ. & Gomot, M. <br>
PsyArXiv Preprints, 2024. <br>
[<img style='display:inline;padding-top: 5px;' height='25' src='/images/site_icons/access.jpg'>]({{site.baseurl}}/articles/2024/merchie_psyarxiv_2024.pdf)


<hr>

### 2023

[_Pupil dilation reflects the dynamic integration of audiovisual emotional speech_](https://www.nature.com/articles/s41598-023-32133-2) <br>
Arias Sarah, P., Hall, L., Saitovitch, A., *Aucouturier, J. J.*, Zilbovicius, M., & Johansson, P. <br>
Scientific reports, 13(1), 5507, 2023. <br>
[<img style='display:inline;padding-top: 5px;' height='25' src='/images/site_icons/access.jpg'>]({{site.baseurl}}/articles/2023/arias_scireport_2023.pdf)


[_Algorithmic voice transformations reveal the phonological basis of language-familiarity effects in cross-cultural emotion judgments_](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0285028) <br>
Nakai, T., Rachman, L., Arias Sarah, P., Okanoya, K., & *Aucouturier, J.J.* <br>
Plos one, 18(5), e0285028, 2023. <br>
[<img style='display:inline;padding-top: 5px;' height='25' src='/images/site_icons/access.jpg'>]({{site.baseurl}}/articles/2023/nakai_plos_one_2023.pdf)


[_Combining GAN with reverse correlation to construct personalized facial expressions_](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0290612) <br>
Yan, S., Soladié, C., *Aucouturier, J. J.*, & Seguier, R. <br>
Plos one, 18(8), e0290612, 2023. <br>
[<img style='display:inline;padding-top: 5px;' height='25' src='/images/site_icons/access.jpg'>]({{site.baseurl}}/articles/2023/sen_plos_one_2023.pdf)

[_The implicit influence of pitch contours and emotional timbre on P300 components in an own-name oddball paradigm_](https://www.biorxiv.org/content/10.1101/2023.11.30.569381v1) <br>
*Pruvost-Robieux, E.*, *Joucla, C.*, *Benghanem, S.*, *Guha, R.*, Liuni, M., Gavaret, M. & *Aucouturier, JJ*<br>
bioRxiv 2023.11.30.569381. <br>
[<img style='display:inline;padding-top: 5px;' height='25' src='/images/site_icons/access.jpg'>]({{site.baseurl}}/articles/2023/pruvost_robieux_bioRxiv_2023.pdf)


[_The psychophysics of empathy: Using reverse-correlation to quantify the overlap between self & other representations of emotional expressions_](https://osf.io/preprints/psyarxiv/rdmve) <br>
Zaied, S, Soladié, C. & *Aucouturier, J.J.* <br>
PsyArXiv rdmve, 2023. <br>
[<img style='display:inline;padding-top: 5px;' height='25' src='/images/site_icons/access.jpg'>]({{site.baseurl}}/articles/2023/zaied_psyarxiv_2023.pdf)
[<img style='display:inline;padding-top: 5px;' height='25' src='/images/site_icons/data.jpg'>](https://github.com/neuro-team-femto/empathy)
[<img style='display:inline;padding-top: 5px;' height='25' src='/images/site_icons/code.jpg'>](https://github.com/neuro-team-femto/empathy)


[_Cracking the pitch code of music-motor synchronization using data-driven methods_](https://osf.io/preprints/psyarxiv/zkbn3) <br>
Migotti, L., *Decultot, Q.*, Grailhe, P. & *Aucouturier, J. J.* <br>
PsyArXiv zkbn3, 2023. <br>
[<img style='display:inline;padding-top: 5px;' height='25' src='/images/site_icons/access.jpg'>]({{site.baseurl}}/articles/2023/migotti_psyarxiv_2023.pdf)
[<img style='display:inline;padding-top: 5px;' height='25' src='/images/site_icons/data.jpg'>](https://github.com/neuro-team-femto/treadmill)
[<img style='display:inline;padding-top: 5px;' height='25' src='/images/site_icons/code.jpg'>](https://github.com/neuro-team-femto/treadmill)


### 2022

[_Three simple steps to improve the interpretability of EEG-SVM studies_](https://www.biorxiv.org/content/10.1101/2021.12.14.472588v1)<br>
*Coralie Joucla*, Damien Gabriel, Juan-Pablo Ortega & Emmanuel Haffen<br>
Journal of Neurophysiology, 2022 <br>
[<img style='display:inline;padding-top: 5px;' height='25' src='/images/site_icons/access.jpg'>]({{site.baseurl}}/articles/2022/Joucla_Biorxiv_2022.pdf)

[_It’s not what you say, it’s how you say it: a retrospective study of the impact of prosody on own-name P300 in comatose patients_](https://www.sciencedirect.com/science/article/pii/S1388245722000128?via%3Dihub)<br>
*Estelle Pruvost-Robieux*, Nathalie André-Obadia, Angela Marchi, Tarek Sharshar, Marco Liuni, Martine Gavaret & *Jean-Julien Aucouturier*<br>
Clinical Neurophysiology, vol. 135, 2022 <br>
[<img style='display:inline;padding-top: 5px;' height='25' src='/images/site_icons/access.jpg'>]({{site.baseurl}}/articles/2022/Pruvost-Robieux_Clinical_Neurophysiology_2022.pdf)

<hr>

### 2021

[_The shallow of your smile: the ethics of expressive vocal deep-fakes_](https://royalsocietypublishing.org/doi/10.1098/rstb.2020.0396) <br>
*Nadia Guerouaou*, Guillaume Vaiva & *JJ Aucouturier*<br>
Philosophical Transations of the Royal Society B, vol. 377 (1841), 2021 <br>
[<img style='display:inline;padding-top: 5px;' height='25' src='/images/site_icons/access.jpg'>]({{site.baseurl}}/articles/2021/Guerouaou_Philosophical_Transactions_2021.pdf)
[<img style='display:inline;padding-top: 5px;' height='25' src='/images/site_icons/data.jpg'>](https://github.com/creamlab/deep-ethics)
[<img style='display:inline;padding-top: 5px;' height='25' src='/images/site_icons/code.jpg'>](https://github.com/creamlab/deep-ethics)

[_Even violins can cry: specifically vocal emotional behaviours also drive the perception of emotions in non-vocal music_](https://royalsocietypublishing.org/doi/10.1098/rstb.2020.0396)<br>
*Daniel Bedoya*, *Pablo Arias*, *Laura Rachman*, Marco Liuni, Clément Canonne, *Louise Goupil* & *JJ Aucouturier*<br>
Philosophical Transations of the Royal Society B, vol. 376(1840), 2021 <br>
[<img style='display:inline;padding-top: 5px;' height='25' src='/images/site_icons/access.jpg'>]({{site.baseurl}}/articles/2021/Bedoya_Philosophical_Transactions_2021.pdf)
[<img style='display:inline;padding-top: 5px;' height='25' src='/images/site_icons/data.jpg'>](http://dx.doi.org/doi:10.25666/DATAOSU-2022-02-28)
[<img style='display:inline;padding-top: 5px;' height='25' src='/images/site_icons/code.jpg'>](https://github.com/creamlab/smiling_violins)

[_Facial mimicry in the congenitally blind_](https://www.cell.com/current-biology/fulltext/S0960-9822(21)01195-7)<br>
*Pablo Arias*, Caren Bellmann & *JJ Aucouturier*<br>
Current Biology, vol. 31(19), PR1112-R1114 (2021) <br>
[<img style='display:inline;padding-top: 5px;' height='25' src='/images/site_icons/access.jpg'>]({{site.baseurl}}/articles/2021/Arias_Current_Biology_2021.pdf)

[_Distinct signatures of subjective confidence and objective accuracy in speech prosody_](https://www.sciencedirect.com/science/article/abs/pii/S0010027721000809)<br>
*Louise Goupil* & *JJ Aucouturier*<br>
Cognition, vol. 212, 104661 (2021) <br>
[<img style='display:inline;padding-top: 5px;' height='25' src='/images/site_icons/access.jpg'>]({{site.baseurl}}/articles/2021/Goupil_Cognition_2021.pdf)

[_Vocal signals only impact speakers’ own emotions when they are self-attributed_](https://www.sciencedirect.com/science/article/abs/pii/S1053810020305390?dgcid=coauthor)<br>
*Louise Goupil*, Petter Johansson, Lars Hall & *JJ Aucouturier* <br>
Consciousness & Cognition, vol. 88, 103072 (2021)<br>
[<img style='display:inline;padding-top: 5px;' height='25' src='/images/site_icons/access.jpg'>]({{site.baseurl}}/articles/2021/Goupil_Consciousness_Cognition_2021.pdf)

[_Listeners perception of certainty and honesty of another speaker is associated with a common prosodic signature_](https://www.nature.com/articles/s41467-020-20649-4)<br>
*Louise Goupil*, *Emmanuel Ponsot*, Daniel Richardson, Gabriel Reyes & *JJ Aucouturier*<br>
Nature Communications, vol. 12, 861 (2021)<br>
[<img style='display:inline;padding-top: 5px;' height='25' src='/images/site_icons/access.jpg'>]({{site.baseurl}}/articles/2021/Goupil_Nature_Communications_2021.pdf)


<hr>

### 2016 - 2020

*Note:*  Articles published before 2020 correspond to work conducted in the [CREAM music neuroscience team](https://cream.ircam.fr) in IRCAM. We list here a selection of publications that are important to our current research. For a complete list of publications on music cognition and vocal emotions from the CREAM team (2016-2020), see the [CREAM archive page]({{site.baseurl}}/cream). For even earlier work on machine learning and audio signal processing, see JJA's [Google Scholar page](https://scholar.google.com/citations?user=jnST06UAAAAJ). 

[_CLEESE: An open-source audio-transformation toolbox for data-driven experiments in speech and music cognition_](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0205943)<br>
*Juan Jose Burred*, *Emmanuel Ponsot*, *Louise Goupil*, *Marco Liuni* & *JJ Aucouturier*<br>
PLoS one, 14(4), e0205943, 2019<br>
[<img style='display:inline;padding-top: 5px;' height='25' src='/images/site_icons/access.jpg'>]({{site.baseurl}}/articles/2019/Burred_PLOS_One_2019.pdf)

[_Cracking the social code of speech prosody using reverse correlation_](https://www.pnas.org/content/115/15/3972)<br>
*Emmanuel Ponsot*, *Juan Jose Burred*, Pascal Belin & *JJ Aucouturier*<br>
Proceedings of the National Academy of Sciences, vol 115 (15) 3972-3977, 2018<br>
[<img style='display:inline;padding-top: 5px;' height='25' src='/images/site_icons/access.jpg'>]({{site.baseurl}}/articles/2018/Ponsot_PNAS_2018.pdf)

[_Uncovering mental representations of smiled speech using reverse correlation_](https://asa.scitation.org/doi/10.1121/1.5020989)<br>
*Emmanuel Ponsot*, *Pablo Arias* & *JJ Aucouturier*<br>
Journal of the Acoustical Society of America, vol 143 (1), 2018.<br>
[<img style='display:inline;padding-top: 5px;' height='25' src='/images/site_icons/access.jpg'>]({{site.baseurl}}/articles/2018/Ponsot_JASA_2018.pdf)

[_DAVID: An open-source platform for real-time transformation of infra-segmental emotional cues in running speech_](https://link.springer.com/article/10.3758/s13428-017-0873-y)<br>
*Laura Rachman*, *Marco Liuni*, *Pablo Arias*, Andreas Lind, Petter Johansson, Lars Hall, Daniel Richardson, Katsumi Watanabe, Stéphanie Dubal & *JJ Aucouturier*<br>
Behaviour Research Methods, vol. 50(1), 323–343, 2017<br>
[<img style='display:inline;padding-top: 5px;' height='25' src='/images/site_icons/access.jpg'>]({{site.baseurl}}/articles/2017/Rachman_BRM_2017.pdf)

[_Covert Digital Manipulation of Vocal Emotion Alter Speakers’ Emotional State in a Congruent Direction_](https://www.pnas.org/content/113/4/948)<br>
*JJ Aucouturier*, Petter Johansson, Lars Hall, Rodrigo Segnini, Lolita Mercadié & Katsumi Watanabe<br>
Proceedings of the National Academy of Sciences, vol. 113 no. 4, 2016<br>
[<img style='display:inline;padding-top: 5px;' height='25' src='/images/site_icons/access.jpg'>]({{site.baseurl}}/articles/2016/Aucouturier_PNAS_2016.pdf)


<hr>

### Copyright Notice

The documents listed here are available for downloading and have been provided as a means to ensure timely dissemination of scholarly and technical work on a noncommercial basis. Copyright and all rights therein are maintained by the authors or by other copyright holders, notwithstanding that they have offered their works here electronically. It is understood that all persons copying this information will adhere to the terms and constraints invoked by each author's copyright. These works may not be re-posted without the explicit permission of the copyright holder.
